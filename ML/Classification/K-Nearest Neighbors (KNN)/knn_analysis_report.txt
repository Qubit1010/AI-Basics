
================================================================================
KNN CLASSIFICATION - WINE QUALITY PREDICTION
================================================================================

BUSINESS OBJECTIVE
================================================================================
Classify red wine quality (LOW / MEDIUM / HIGH) from chemical lab measurements,
enabling wineries to predict quality before expensive human tasting panels,
flag defective batches early, and optimize production chemistry.

WHY KNN FOR WINE QUALITY?
  - Wines with similar chemistry taste similar (locality = prediction)
  - Multi-class problem handled naturally (majority vote among K neighbors)
  - No assumption about data distribution
  - Interpretable: prediction explained by citing similar wines
  - Chemical similarity IS the domain logic

DATASET SUMMARY
================================================================================
  Total wine samples: 1500
  Features:           11
  LOW quality:        300 wines (20.0%)
  MEDIUM quality:     900 wines (60.0%)
  HIGH quality:       300 wines (20.0%)

  Train / Test split: 80% / 20% (stratified)

KEY CHEMICAL DIFFERENCES BY TIER
================================================================================
             FixedAcidity  VolatileAcidity  CitricAcid  ResidualSugar  Chlorides  FreeSulfurDioxide  TotalSulfurDioxide  Density     pH  Sulphates  Alcohol
QualityTier                                                                                                                                                
HIGH                8.838            0.334       0.425          2.752      0.072             13.451              33.923    0.995  3.254      0.785   11.811
LOW                 7.494            0.747       0.187          2.627      0.113             11.274              34.754    0.997  3.459      0.522    9.762
MEDIUM              8.180            0.496       0.295          2.582      0.085             16.318              46.040    0.996  3.314      0.652   10.608

OPTIMAL K SELECTION
================================================================================
  Method:     5-fold cross-validation across K=1 to K=50
  Best K:     18
  CV Accuracy at best K: 0.9325
  Rule-of-thumb sqrt(n_train) = 34

K SENSITIVITY ANALYSIS
================================================================================
  K=1   (Overfit)  -> Memorizes training, poor generalization
  K=18   (Optimal) -> Best balance, highest CV accuracy
  K=50  (Underfit) -> Over-smoothed, misses patterns

DISTANCE METRIC COMPARISON (K=18)
================================================================================
  Metric            Test Acc   F1-Score     CV Acc
  -----------------------------------------------
  Euclidean           0.9067     0.9062     0.9325
  Manhattan           0.9067     0.9065     0.9283
  Minkowski           0.8933     0.8925     0.9225
  Chebyshev           0.9033     0.9026     0.9142
  Best Metric: Euclidean

FINAL MODEL PERFORMANCE (K=18, Euclidean)
================================================================================
  Test Accuracy:   0.9067  (90.67%)
  Weighted F1:     0.9062

  Classification Report:
              precision    recall  f1-score   support

        HIGH       0.88      0.85      0.86        60
         LOW       0.94      0.85      0.89        60
      MEDIUM       0.90      0.94      0.92       180

    accuracy                           0.91       300
   macro avg       0.91      0.88      0.89       300
weighted avg       0.91      0.91      0.91       300


  Confusion Matrix (rows=Actual, cols=Predicted):
                LOW   MEDIUM     HIGH
  LOW           51        9        0
  MEDIUM         3      170        7
  HIGH           0        9       51

CROSS-VALIDATION (5-Fold Stratified)
================================================================================
  Accuracy: 0.9283 +/- 0.0096
  F1-Score: 0.9277 +/- 0.0095

KNN ADVANTAGES IN THIS SCENARIO
================================================================================
  - No model training: stores all samples, predicts by similarity
  - Naturally multi-class: voting among K neighbors
  - Non-parametric: no assumption about decision boundary shape
  - Local patterns captured: each region of chemical space independent
  - Probability output: confidence score per prediction

KNN LIMITATIONS
================================================================================
  - Slow at prediction time: computes distance to ALL training wines
  - Memory-heavy: must store entire dataset
  - Sensitive to irrelevant features (all features used in distance)
  - Needs feature scaling (critical!)
  - Performance degrades in very high dimensions

BUSINESS RECOMMENDATIONS
================================================================================
  1. Deploy at bottling stage: run chemical analysis, get quality prediction
  2. Flag LOW quality batches for blending or re-processing
  3. Prioritize HIGH quality batches for premium labeling
  4. Monitor volatile acidity closely (strongest LOW quality signal)
  5. Optimize alcohol levels (strongest HIGH quality signal)
  6. Retrain model quarterly as new vintage data arrives

FILES GENERATED
================================================================================
  Data:
    wine_quality_data.csv

  Visualizations:
    knn_viz_1_distribution.png   - Class distribution + key features
    knn_viz_2_elbow.png          - Optimal K elbow curve
    knn_viz_3_confusion.png      - Confusion matrix
    knn_viz_4_pca_boundary.png   - Decision boundary in PCA space
    knn_viz_5_boxplots.png       - Chemical features by tier
    knn_viz_6_metrics.png        - Distance metric comparison
    knn_viz_7_confidence.png     - Prediction confidence analysis

================================================================================
END OF REPORT
================================================================================
