
================================================================================
REINFORCEMENT LEARNING — UPPER CONFIDENCE BOUND (UCB)
SCENARIO: COFFEE SHOP DAILY PROMOTION SELECTION
================================================================================

BUSINESS OBJECTIVE
================================================================================
A coffee shop shows ONE promotional banner per day on their app/website.
They have 5 promotions but don't know which converts best.
Goal: Maximize total customer responses over 500 days.
Challenge: Explore options vs exploit known winners (Exploration-Exploitation tradeoff).

SCENARIO SETUP
================================================================================
Total Days (Rounds): 500
Promotions: 5

Promo   Name                            True Rate   Role
-----------------------------------------------------------------
  P0    10% OFF all drinks              0.35        Competitor
  P1    Buy 2 get 1 FREE pastry         0.50        Competitor
  P2    Free size upgrade latte         0.25        Competitor
  P3    Loyalty double points           0.40        Competitor
  P4    Free cookie with coffee         0.60        ★ TRUE BEST (hidden)

UCB ALGORITHM EXPLANATION
================================================================================
Upper Confidence Bound (UCB) solves the Multi-Armed Bandit problem.

Formula:
  UCB_i(t) = average_reward_i + sqrt( 2 * ln(t) / N_i(t) )

Components:
  average_reward_i  = empirical mean reward of promo i
  sqrt(2*ln(t)/N_i) = confidence bonus (uncertainty term)
  t                 = current round
  N_i               = times promo i has been selected

Intuition:
  - New / rarely tried promo → large confidence bonus → gets explored
  - Frequently tried promo   → small confidence bonus → selected only if truly good
  - Algorithm auto-balances exploration vs exploitation mathematically

The key property: UCB is OPTIMISTIC — it assumes an arm COULD be as good as its
upper confidence bound allows. This "optimism in the face of uncertainty" drives
systematic exploration.

EXECUTION RESULTS
================================================================================

Promo   Name                            Selected    % Days    Est. Rate     True Rate   Diff
------------------------------------------------------------------------------------------
  P0    10% OFF all drinks              52          10.4      0.3462        0.35        -0.0038
  P1    Buy 2 get 1 FREE pastry         89          17.8      0.4494        0.50        -0.0506
  P2    Free size upgrade latte         44          8.8       0.2955        0.25        +0.0455
  P3    Loyalty double points           83          16.6      0.4337        0.40        +0.0337
  P4    Free cookie with coffee         232         46.4      0.5948        0.60        -0.0052 ★ WINNER

UCB vs RANDOM COMPARISON
================================================================================
Metric                                      UCB      Random  UCB Advantage
--------------------------------------------------------------------------
Total Customer Responses                    245         221            +24
Total Regret                              53.90       89.65         +35.75
Avg Daily Reward                         0.4900      0.4420        +0.0480
Efficiency vs Optimal                     81.7%       73.7%               
Optimal Promo Selected                      232         102               
% Time on Best Promo                      46.4%       20.4%               

KEY INSIGHTS
================================================================================
1. UCB earned 24 more customer responses than random selection over 500 days.
2. UCB spent 46.4% of days on the true best promo (P4).
   Random only spent ~20% on it (uniform distribution expected).
3. UCB's cumulative regret grows sub-linearly — it gets smarter every day.
4. Random's regret grows linearly — it never learns, wastes opportunities forever.
5. UCB correctly estimated all true conversion rates within normal variance.

EXPLORATION vs EXPLOITATION BALANCE
================================================================================
Early Phase (Day 1-5):
  All promos tried once (forced exploration to avoid division by zero).
  UCB scores artificially high for untried promos.

Middle Phase (Day 5-~100):
  UCB actively explores all promos, confidence bounds shrinking.
  Gradually identifies P4 as best candidate.

Late Phase (Day ~100-500):
  UCB converges to exploit Promo 4 most of the time.
  Occasionally re-explores others (if confidence bounds warrant it).
  Near-optimal performance achieved.

BUSINESS RECOMMENDATIONS
================================================================================
1. DEPLOY UCB FOR PROMO SELECTION
   → Run the UCB algorithm on real customer click data.
   → After ~100 days, it will have converged to the best promo.
   → Continue running: it adapts if promo performance changes seasonally.

2. IMMEDIATE WINNER: Promo 4 — "Free cookie with coffee"
   → UCB identified this as the best performer.
   → Estimated conversion rate: 0.5948 (true: 0.60)

3. AVOID RANDOM ROTATION
   → Randomly cycling through promos wastes ~36 customer interactions.
   → UCB achieves 81.7% of theoretical maximum efficiency.

4. SCALE THE APPROACH
   → Apply UCB to: email subject lines, push notification timing,
     product recommendations, pricing tiers, UI layout variants.
   → Any decision with uncertain, learnable outcomes benefits from UCB.

ADVANTAGES OF UCB OVER ALTERNATIVES
================================================================================
vs Random Selection:
  + Learns over time, random never improves
  + 10.9% more rewards over 500 days
  + Guaranteed sub-linear regret growth

vs Epsilon-Greedy:
  + No fixed epsilon to tune
  + Principled confidence interval approach
  + Automatically reduces exploration as certainty increases

vs Thompson Sampling:
  + Deterministic (same input = same output)
  + Easier to explain to stakeholders
  + Mathematically proven regret bounds

THEORETICAL PROPERTIES
================================================================================
UCB Regret Bound:
  Cumulative Regret ≤ O(sqrt(K * T * ln(T)))
  where K = 5 promos, T = 500 days

  Our actual UCB regret: 53.90
  Random regret (linear): 89.65

UCB is asymptotically optimal — no algorithm can have significantly
lower regret than UCB over the long run.

FILES GENERATED
================================================================================
  • ucb_viz_1_cumulative_reward.png    — UCB vs Random cumulative reward
  • ucb_viz_2_cumulative_regret.png    — Regret growth comparison
  • ucb_viz_3_selection_timeline.png   — Which promo selected each day
  • ucb_viz_4_selection_distribution.png — UCB vs Random selection counts
  • ucb_viz_5_ucb_dynamics.png         — UCB scores + reward convergence
  • ucb_viz_6_dashboard.png            — Full performance dashboard

================================================================================
END OF REPORT
================================================================================
